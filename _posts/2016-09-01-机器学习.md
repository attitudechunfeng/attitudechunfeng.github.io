---
layout:     post
title:      "机器学习"
subtitle:   "机器学习面试要点"
date:       2016-09-01
author:     "attitudechunfeng"
header-img: "img/home-bg.jpg"
---

### 					机器学习面试要点总结

------

##### [1] 常见聚类方法有哪些类？ 

答：基于划分的聚类(K-means聚类)，层次聚类，基于密度的聚类（DBSCAN）,基于网格的聚类算法和基于模型的聚类算法。

- K-means优缺点：

  K-means算法缺点主要是：对异常值敏感；需要提前确定k值；对簇的形状敏感。优点：计算时间短，速度快；容易解释；聚类效果还不错。


- 层次聚类不用确定k值，对簇形状不敏感。大数据下运算慢。


- 基于密度的聚类对噪声点的容忍性非常好；对簇的形状不敏感。缺点是需要用户输入参数，对参数敏感。

评价聚类效果：总体思想为一个cluster聚类内的数据点聚集在一起的密度越高，圈子越小，离centroid中心点越近，那么这个聚类的总体质量相对来说就会越好。

------

**[2] 决策树中，ID3和C4.5分别如何选择属性？**

答：ID3通过计算属性的信息增益来进行属性选择，ID3算法的所选的分类属性偏向于特征值数目多的分类属性。C4.5通过计算属性的信息增益率来进行属性选择。当决策树不能继续进行分裂，但是其中又含有不止一个类别的数据时，则一般是通过选择该节点中，同类最多的数据的属性作为该节点的属性。

------

**[3] 决策树优缺点**： 
优点： 

1) 可以生成可以理解的规则。 

2) 计算量相对来说不是很大。 

3) 可以处理连续和种类字段。 

4) 决策树可以清晰的显示哪些字段比较重要 

缺点： 

1) 对连续性的字段比较难预测。 

2) 对有时间顺序的数据，需要很多预处理的工作。 

3) 当类别太多时，错误可能就会增加的比较快。 

4) 一般的算法分类的时候，只是根据一个字段来分类。

------

**[4] L1正则和L2正则化的区别：**

答：L1优点是能够获得sparse模型，得到稀疏的权值，对于large-scale的问题来说这一点很重要，因为可以减少存储空间。缺点是加入L1后目标函数在原点不可导，需要做特殊处理。
L2优点是实现简单，能够起到正则化的作用，得到平滑的权值。缺点就是L1的优点：无法获得sparse模型。
实际上L1也是一种妥协的做法，要获得真正sparse的模型，要用L0正则化。

------

**[5] 线性回归（Linear Regression）**：

损失函数：
$$
J(\theta)=\frac{1}{2}\sum_{i=1}^{m}{({h}_{\theta}({x}^{(i)})-{y}^{(i)})}^{2}
$$
梯度计算：
$$
\frac{\partial}{{\partial\theta}_{j}}J(\theta)=\sum_{i=1}^{m}({h}_{\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{(i)}
$$
标准梯度下降：Repeat until convergence：{
$$
\theta_{j}:=\theta_{j}+\alpha\sum_{i=1}^{m}({h}_{\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{(i)}
$$
}

随机梯度下降法（SGD）: LOOP{

​	for i=1 to m, {
$$
\theta_{j}:=\theta_{j}+\alpha({h}_{\theta}({x}^{(i)})-{y}^{(i)})x_{j}^{(i)}
$$
​	}

}

最小二乘法：
$$
X^{T}X\theta=X^{T}\vec{y}
$$

$$
\theta=(X^{T}X)^{-1}X^{T}\vec{y}
$$

损失函数的概率角度证明：
$$
y^{(i)}=\theta^{T}x^{(i)}+\epsilon^{(i)}
$$
假设误差项服从高斯分布，均值为0，方差为σ2，则有
$$
p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^{2}}{2\sigma^{2}})
$$
假设各样本满足独立同分布，则有
$$
p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma^{2}})
$$
似然函数为
$$
L(\theta)=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma^{2}})
$$
log似然函数为
$$
l(\theta)=logL(\theta)=mlog\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^{2}}\sum_{i=1}^{m}(y^{(i)}-\theta^{T}x^{(i)})^{2}
$$
因此最大化上述函数等同于最小化后面的公式项，即线性回归的损失函数。

------

**[6] logistic regression：**
$$
g(z)=\frac{1}{1+e^{-z}}
$$

$$
h_{\theta}(x)=g(\theta^{T}x)=\frac{1}{1+e^{-\theta^{T}x}}
$$

$$
g^{'}(z)=\frac{d}{dz}\frac{1}{1+e^{-z}}=g(z)(1-g(z))
$$

假定
$$
P(y=1|x;\theta)=h_{\theta}(x)
$$

$$
P(y=0|x;\theta)=1-h_{\theta}(x)
$$

统一写法为
$$
p(y|x;\theta)=h_{\theta}(x)^{y}(1-h_{\theta}(x))^{1-y}
$$
log似然函数为
$$
l(\theta)=\sum_{i=1}^{m}y^{(i)}logh_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))
$$
随机梯度下降法求解最大似然函数，对于每个样本每个参数，求解偏导
$$
\begin{aligned}
\frac{\partial}{\partial\theta_{j}}l(\theta)&=(\frac{y}{h_{\theta}(x)}-\frac{1-y}{1-h_{\theta}(x)})\frac{\partial h_{\theta}(x)}{\partial \theta_{j}}\\&=(\frac{y}{h_{\theta}(x)}-\frac{1-y}{1-h_{\theta}(x)})h_{\theta}(x)(1-h_{\theta}(x))\frac{\partial \theta^{T}x}{\partial \theta_{j}}\\
&=(y(1-h_{\theta}(x))-(1-y)h_{\theta}(x))x_{j} \\
&=(y-h_{\theta}(x))x_{j}
\end{aligned}
$$

$$
\theta_{j}:=\theta_{j}+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_{j}^{(i)}
$$

------

**[7] 牛顿法求解**
$$
\begin{aligned}f(\theta)=0\end{aligned}
$$

$$
\theta:=\theta+\frac{f(\theta)}{f^{'}(\theta)}
$$

对于最大化似然函数目标变为求解
$$
l^{'}(\theta)=0
$$

$$
\theta:=\theta+\frac{l^{'}(\theta)}{l^{''}(\theta)}
$$

对于logistic regression迭代公式为
$$
\theta:=\theta-H^{-1}\nabla_{\theta}l(\theta)\\
其中\nabla_{\theta}l(\theta)为l(\theta)对每一个参数所求偏导组成的向量，\\
H_{ij}=\frac{\partial^{2}l(\theta)}{\partial\theta_{i}\partial\theta_{j}}
$$

------

**[8] 朴素贝叶斯：**
$$
bayes法则：p(y|x)=\frac{p(x|y)p(y)}{p(x)}
$$

$$
\begin{aligned}\mathop{argmax}_{y}p(y|x)&=\frac{\mathop{argmax}_{y}p(x|y)p(y)}{p(x)}\\
&=\mathop{argmax}_{y}p(x|y)p(y)\end{aligned}
$$

**朴素贝叶斯假设**：给定y的条件下，各属性间条件独立。

**拉普拉斯平滑：**当测试样本中的某一属性值从未在训练集中出现时，应用贝叶斯公式会出现分子或分母为0的情况。解决方法是在分子分母上同时加上实数，即拉普拉斯平滑。

------

**[9] SVM:**

支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
$$
对于二分类任务（分类标签y为-1和1），线性分类器方程：w^{T}x+b=0
$$

$$
函数间隔：\hat{r}=y(w^{T}x+b)
$$

$$
\hat{r}=min\hat{r}_{i} (i=1...n)
$$

但这样定义的函数间隔有问题，即如果成比例的改变w和b（如将它们改成2w和2b），则函数间隔的值f(x)却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够。
事实上，我们可以对法向量w加些约束条件，从而引出真正定义点到超平面的距离--几何间隔（geometrical margin）的概念。
$$
\tilde{r}=\frac{\hat{r}}{||w||}
$$
于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为：
$$
max\tilde{r}
$$
同时需满足一些条件，根据间隔的定义，有
$$
y_{i}(w^{T}x_{i}+b)=\tilde{r}_{i}>=\tilde{r},    i=1...n
$$
由于w和b任意改变相同倍数的大小对结果无影响，因此这里任意改变w和b的尺度，使得训练集中的每个样本都满足
$$
\hat{r}=1
$$
于是，上述目标函数转化为
$$
max\frac{1}{||w||}, s.t.,y_{i}(w^{T}x_{i}+b)>=1, i=1...n\\等价于优化函数\\
min\frac{1}{2}||w||^{2}, s.t.,y_{i}(w^{T}x_{i}+b)>=1, i=1...n
$$
因为现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。这个问题可以用现成的QP (Quadratic Programming) 优化包进行求解。一言以蔽之：在一定的约束条件下，目标最优，损失最小。

此外，由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法，**<u>这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。</u>**
$$
L(w,b,\alpha)=\frac{1}{2}||w||^{2}-\sum_{i=1}^{n}\alpha_{i}(y_{i}(w^{T}x_{i}+b)-1)
$$
然后令
$$
\theta(w)=\mathop{max}_{\alpha_{i}>=0}L(w,b,\alpha)\\
容易验证，当某个约束条件不满足时，例如y_{i}(w^{T}x_{i}+b)<1,显然有\theta(w)=\infty(只要令\alpha_{i}=\infty即可)。\\而当所有约束条件都满足时，则最优值为\theta(w)=\frac{1}{2}||w||^{2},亦即最初要最小化的量。\\因此，在要求约束条件得到满足的情况下最小化\frac{1}{2}||w||^{2}实际上等价于直接优化\theta(w)\\(约束条件为\alpha_{i}>=0,i=1...n),因为如果约束条件没有满足，\theta(w)会无穷大，自然不会是我们要求的\\最小值。
$$
目标函数变为
$$
\mathop{\min}_{w,b}\theta(w)=\mathop{\min}_{w,b}\mathop{\max}_{\alpha_{i}>=0}L(w,b,\alpha)=p^{*}
$$
如果直接求解，那么一上来便得面对*w*和*b*两个参数，而*αi*又是不等式约束，这个求解过程不好做。不妨把最小和最大的位置交换一下，变成
$$
\mathop{\max}_{\alpha_{i}>=0}\mathop{\min}_{w,b}L(w,b,\alpha)=d^{*}
$$
交换以后的新问题是原始问题的对偶问题，而且有*d\*≤p\**，在满足**<u>KKT</u>**条件的情况下，这两者相等，这个时候就可以通过求解对偶问题来间接地求解原始问题。

换言之，之所以从minmax的原始问题，转化为maxmin的对偶问题，一者因为d\*是*p\**的近似解，二者，转化为对偶问题后，更容易求解。

下面可以先求*L* 对*w*、*b*的极小，再求*L* 对*α*的极大。
$$
\begin{aligned}\frac{\partial L}{\partial w}=0\Rightarrow w=\sum_{i=1}^{n}\alpha_{i}y_{i}x_{i}\end{aligned}\\\frac{\partial L}{\partial b}=0\Rightarrow \sum_{i=1}^{n}\alpha_{i}y_{i}=0
$$
将以上结果带入*L*得
$$
\begin{aligned}L(w,b,\alpha)&=\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}-\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}-b\sum_{i=1}^{n}\alpha_{i}y_{i}+\sum_{i=1}^{n}\alpha_{i}\\&=\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}\end{aligned}
$$
求对*α*的极大，即是关于对偶问题的最优化问题。经过上面第一个步骤的求*w*和*b*，得到的拉格朗日函数式子已经没有了变量*w*，*b*，只有*α*。从上面的式子得到
$$
\begin{aligned}&\mathop{\max}_{\alpha}\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}\\&s.t.,\alpha_{i}>=0,i=1...n\\&\sum_{i=1}^{n}\alpha_{i}y_{i}=0\end{aligned}
$$
这样，在求解出*α*之后，就可以得到*w*和*b*。最后一步则可以利用SMO算法求解对偶问题中的拉格朗日乘子*α*。

**线性不可分**

从前面推导中我们得到
$$
w=\sum_{i=1}^{n}\alpha_{i}y_{i}x_{i}
$$
因此，分类函数为
$$
f(x)=\sum_{i=1}^{n}\alpha_{i}y_{i}<x_{i},x>+b
$$
这里的形式的有趣之处在于，对于新点 x的预测，只需要计算它与训练数据点的内积即可（<·, ·>表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非Supporting Vector 所对应的系数都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。

    为什么非支持向量对应的α等于零呢？直观上来理解的话，就是这些“后方”的点——正如我们之前分析过的一样，对超平面是没有影响的，由于分类完全由超平面决定，所以这些无关的点并不会参与分类问题的计算，因而也就不会产生任何影响了。

对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。具体来说，在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。

分类函数变为
$$
f(x)=\sum_{i=1}^{n}\alpha_{i}y_{i}\kappa<x_{i},x>+b
$$
其中*α*由以下dual问题求得
$$
\begin{aligned}&\mathop{\max}_{\alpha}\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}\kappa<x_{i},x_{j}>\\&s.t.,\alpha_{i}>=0,i=1...n\\&\sum_{i=1}^{n}\alpha_{i}y_{i}=0\end{aligned}
$$
这样一来计算的问题就算解决了，避开了直接在高维空间中进行计算，而结果却是等价的！**核函数的价值在于它虽然也是讲特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。**

**使用松弛变量处理 outliers 方法**

outlier 偏离了自己原本所应该在的那个半空间，如果直接忽略掉它的话，原来的分隔超平面还是挺好的，但是由于这个 outlier 的出现，导致分隔超平面不得不被挤歪了，同时 margin 也相应变小了。当然，更严重的情况是，对于一些极端的outlier，我们将无法构造出能将数据分开的超平面来。为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。

原来的约束条件为
$$
y_{i}(w^{T}x_{i}+b)>=1,i=1...n
$$
现在考虑到outlier问题，约束条件变成了
$$
y_{i}(w^{T}x_{i}+b)>=1-\xi_{i},i=1...n
$$
其中*ξi*称为松弛变量 (slack variable) ，对应数据点允许偏离的 functional margin 的量。当然，如果我们运行任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些偏离量的总和也要最小
$$
min\frac{1}{2}||w||^{2}+C\sum_{i=1}^{n}\xi_{i}
$$
其中*C*是一个参数，用于控制目标函数中两项（“寻找 margin 最大的超平面”和“保证数据点偏差量最小”）之间的权重。注意，其中ξ是需要优化的变量（之一），而 *C*是一个事先确定好的常量。完整地写出来是这个样子
$$
min\frac{1}{2}||w||^{2}+C\sum_{i=1}^{n}\xi_{i}\\s.t.,y_{i}(w^{T}x_{i}+b)>=1-\xi_{i},i=1...n\\
\xi_{i}>=0,i=1...n
$$
用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示
$$
L(w,b,\alpha,\xi,r)=\frac{1}{2}||w||^{2}+C\sum_{i=1}^{n}\xi_{i}-\sum_{i=1}^{n}\alpha_{i}(y_{i}(w^{T}x_{i}+b)-1+\xi_{i})-\sum_{i=1}^{n}r_{i}\xi_{i}
$$
分析方法和前面一样，转换为另一个问题之后，我们先让*L*针对*w*、*b*和*ξ*最小化
$$
\begin{aligned}\frac{\partial L}{\partial w}=0&\Rightarrow w=\sum_{i=1}^{n}\alpha_{i}y_{i}x_{i}\\\frac{\partial L}{\partial b}=0&\Rightarrow \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
\frac{\partial L}{\partial\xi_{i}}=0&\Rightarrow C-\alpha_{i}-r_{i}=0,i=1...n\end{aligned}
$$
 将*w*带回*L*并化简，得到和原来一样的目标函数
$$
\mathop{\max}_{\alpha}\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}<x_{i},x_{j}>
$$
 不过，由于我们得到*C*-*αi*-*ri*=0而又有*ri*>=0（作为 Lagrange multiplier 的条件），因此有，所以整个 dual 问题现在写作
$$
\begin{aligned}&\mathop{\max}_{\alpha}\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}\kappa<x_{i},x_{j}>\\&s.t.,0=<\alpha_{i}<=C,i=1...n\\&\sum_{i=1}^{n}\alpha_{i}y_{i}=0\end{aligned}
$$
可以看到唯一的区别就是现在 dual variable *α*多了一个上限*C*。而 Kernel 化的非线性形式也是一样的，只要把<*xi*,*xj*>换成*k*<*xi*,*xj*>即可。这样一来，一个完整的，可以处理线性和非线性并能容忍噪音和 outliers 的支持向量机才终于介绍完毕了。

------

**[10] 随机森林：**

随机森林顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输 入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本 为那一类。

在建立每一棵决策树的过程中，有两点需要注意 - 采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那 么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M 个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一 个分类。一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。

- 优点：
1. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择。
2. 由于随机选择样本导致的每次学习决策树使用不同训练集，所以可以一定程度上避免过拟合。
3. 训练速度快。
4. 实现简单，易并行化。

- 缺点：

1. 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。

2. 对于有不同级别的属性的数据，级别划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。

------

**[11] Adaboost:**   

AdaBoost，是英文"Adaptive Boosting"（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

Adaboost的算法流程如下：

首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N。
$$
D_{1}=(w_{11},w_{12}...w_{1i}...,w_{1N}),w_{1i}=\frac{1}{N},i=1...N
$$
进行多轮迭代，用m = 1,2, ..., M表示迭代的第多少轮。

使用具有权值分布Dm的训练数据集学习，得到基本分类器
$$
G_{m}(x):\chi\rightarrow \{-1,+1 \}
$$
计算Gm(x)在训练数据集上的分类误差率
$$
e_{m}=P(G_{m}(x_{i})\neq y_{i})=\sum_{i=1}^{N}w_{mi}I(G_{m}(x_{i})\neq y_{i})
$$
由上述式子可知，Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。

计算Gm(x)的系数，am表示Gm(x)在最终分类器中的重要程度:
$$
\alpha_{m} = \frac{1}{2}log\frac{1-e_{m}}{e_{m}}
$$
由上述式子可知，em <= 1/2时，am >= 0，且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。

更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代
$$
\begin{aligned}D_{m+1}&=(w_{m+1,1},w_{m+1, 2}...w_{m+1,i}...,w_{m+1,N}),\\
w_{m+1,i}&=\frac{w_{mi}}{Z_{m}}exp(-\alpha_{m}y_{i}G_{m}(x_{i})),i=1...N\end{aligned}
$$
使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上。

其中，Zm是规范化因子，使得Dm+1成为一个概率分布：
$$
Z_{m}=\sum_{i=1}^{N}w_{mi}exp(-\alpha_{m}y_{i}G_{m}(x_{i}))
$$
组合各个弱分类器
$$
f(x)=\sum_{m=1}^{M}\alpha_{m}G_{m}(x)
$$
从而得到最终分类器，如下：
$$
G(x)=sign(f(x))=sign(\sum_{m=1}^{M}\alpha_{m}G_{m}(x))
$$

------

**[12] BP网络：**

对于一个三层网络，分别为输入层，隐藏层和输出层。激活函数为sigmoid函数。

当加入第k个样本时，隐蔽层h结点的输入加权和为：
$$
s_{h}^{k} = \sum_{i}w_{ih}x_{i}^{k}
$$
相应点的输出为：
$$
y_{h}^{k}=f(s_{h}^{k})=f(\sum_{i}w_{ih}x_{i}^{k})
$$
同样，输出层j结点的输入加权和为：
$$
s_{j}^{k}=\sum_{h}w_{hj}y_{h}^{k}=\sum_{h}w_{hj}f(\sum_{i}w_{ih}x_{i}^{k})
$$
相应点的输出为：
$$
y_{j}^{k} = f(s_{j}^{k})=f(\sum_{h}w_{hj}f(\sum_{i}w_{ih}x_{i}^{k}))
$$
误差函数为：
$$
E(W)=\frac{1}{2}\sum_{k,j}(T_{j}^{k}-y_{j}^{k})^{2}=\frac{1}{2}\sum_{k,j}\{T_{j}^{k}-f(\sum_{h}w_{hj}f(\sum_{i}w_{ih}x_{i}^{k}))\}^{2}
$$
为了使误差函数最小，用梯度下降法求得最优的加权，权值先从输出层开始修正，然后依次修正前层权值，因此含有反传的含义。

根据梯度下降法，由隐蔽层到输出层的连接的加权调节量为：
$$
\begin{aligned}\Delta w_{hj}&=-\eta\frac{\partial E}{\partial w_{hj}}=\eta\sum_{k}(T_{j}^{k}-y_{j}^{k})f^{'}(s_{j}^{k})y_{h}^{k}=\eta\sum_{k}\delta_{j}^{k}y_{h}^{k}(1)\\
其中\delta_{j}^{k}为输出结点的误差信号：\\
\delta_{j}^{k}&=f^{'}(s_{j}^{k})(T_{j}^{k}-y_{j}^{k})=f^{'}(s_{j}^{k})\Delta_{j}^{k}\\
\Delta_{j}^{k}&=T_{j}^{k}-y_{j}^{k}\end{aligned}
$$
对于输入层到隐蔽层结点连接的加权修正量ΔWih，必须考虑将E(W)对Wih求导，因此利用分层链路法，有：

$$
\begin{aligned}\Delta w_{ih}&=-\eta\frac{\partial E}{\partial w_{ih}}=-\eta\sum_{k}\frac{\partial E}{\partial y_{h}^{k}}\frac{\partial y_{h}^{k}}{\partial w_{ih}}=\eta\sum_{k,j}\{(T_{j}^{k}-y_{j}^{k})f^{'}(s_{j}^{k})w_{hj}f^{'}(s_{h}^{k})x_{i}^{k}\}\\
&=\eta\sum_{k,j}\delta_{j}^{k}w_{hj}f^{'}(s_{h}^{k})x_{i}^{k}=\eta\sum_{k}\delta_{h}^{k}x_{i}^{k}(2)\\
其中：\\
\delta_{h}^{k}&=f^{'}(s_{h}^{k})\sum_{j}w_{hj}\delta_{j}^{k}=f^{'}(s_{h}^{k})\Delta_{h}^{k}\\
\Delta_{h}^{k}&=\sum_{j}w_{hj}\delta_{j}^{k}
\end{aligned}
$$
可以看出，式(1)和(2)具有相同的形式，所不同的是其误差值的定义，所以可定义BP算法对任意层的加权修正量的一般形式：
$$
\Delta_{pq}=\eta\sum_{vector\_no\_P}\delta_{o}y_{in}
$$
若每加入一个训练对所有加权调节一次，则可写成：
$$
\Delta_{pq}=\eta\delta_{o}y_{in}\\
其中，下标o和in指相关连接的输出端点和输入端点，y_{in}代表输入端点的实际输入，δ_{o}表示输出端点\\的误差。如果前面还有隐蔽层，用\delta_{h}^{k}再按上述方法计算\Delta_{l}^{k}和\delta_{l}^{k}，以此类推，一直将输出误差δ一层一\\层推算到第一隐蔽层为止。各层的δ求得后，各层的加权调节量即可按上述公式求得。由于误差\delta_{j}^{k}相当\\于由输出向输入反向传播，所以这种训练算法成为误差反传算法（BP算法）。
$$

------

